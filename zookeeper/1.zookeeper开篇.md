## zookeeper第一讲

### 1. 经典的CAP/BASE理论

#### 1.1 CAP

> **C 一致性 （Consistency）：所有节点上的数据，时刻保持一致。**
>
> **A 可用性（Availability）：每个请求都能够收到一个相应，无论相应成功或者失败。**
>
> **P 分区容错性（Partition-tolerance）：表示系统出现脑裂以后，可能导致某些server与集群中的其他机器失去联系**



**cap理论，不能实现完全拥有CAP三性，只能保证ap/cp的情况：**

* AP【在保持高可用的同时，会失去一致性】；

* CP【在拥有一致性的时候，会失去高可用性】；

`CAP理论仅使用与原子读写的Nosql场景，不适用于数据库系统。`



#### 1.2 BASE

> 基于CAP理论，CAP理论并不适用与数据库事务基于BASE理论，（因为更新一些错误的数据而导致数据出现紊乱，无论什么样的数据高可用方案都是徒劳），
>
> 虽然XA事务可以保证数据库在分布式系统下的ACID特性，但是会带来性能方面的影响；



**eBay尝试了一种完全不同的套路，放宽了对事务ACID的要求，提出了BASE理论。**

> * Basically available （基本可用）： 
>
> ​		数据库采用分片模式，把100w的用户数据分布在5个实例上，如果破坏了其中一个实例，人能固然可以保证80%的用户可用
>
> * soft-state （软状态）：
>
> ​		在基于client-server 模式的系统中，server端是否有状态，决定了系统是否具备良好的水平扩展/负载均衡/故障恢复等特性。
>
> ​		Server端承诺会维护client端状态数据，这个状态仅仅维持一小段时间，这段时间以后，server端就会丢弃这个状态，恢复正常状态。
>
> * Eventually consistent（最终一致性）：
>
> ​		 数据最终一致性



### 2. zookeeper

#### 2.1 zookeeper是什么

`zookeeper 是  分布式数据一致性  的解决方案`

**zookeeper应用场景：**

> 数据发布/订阅（配置中心：disconf），
>
> 负载均衡（dubbo利用了zookeeper机制实现负载均衡），
>
> 命名服务，
>
> master选举（kafka/hadoop/habase），
>
> 分布式队列，
>
> 分布式锁



#### 2.2 zookeeper的特性

> * 顺序一致性
>
>   ​		从同一个客户端发起的事务请求，最终会严格按照顺序被应用到zookeeper中。
>
> * 原子性
>
>   ​		所有的事务请求的处理结果在整个集群中的所有机器上的应用情况是一致的，也就是说，要么整个集群中的所有机器都成功应用了某一事物，要么全都不应用。
>
> * 可靠性
>
>   ​		一旦服务器成功应用了某一个事务数据，并且对客户端做了相应，那么这个数据在整个集群中一定是同步并且保留下来的。
>
> * 实时性
>
>   ​		一旦一个事务被成功应用，客户端就能够立即从服务器端读取到事务变更后的最新数据状态；（zookeeper仅仅保证在一定时间内，接近实时）



#### 2.3 zookeeper数据模型

`zookeeper数据模型`

> zookeeper的数据模型和文件系统类似，每一个节点成为：znode，是zookeeper中的最小数据单元。每个znode上都可以保存数据和挂载子节点。从而构成一个层次化的属性结构。

`数据模型上的每个znode特点：`

> 持久化节点 ：节点创建后会一直存在zookeeper服务器上，直到主动删除。
>
> 持久化有序节点 ： 每个节点都会为他的以及子节点维护一个顺序
>
> 临时节点 ：临时节点的生命周期和客户端的会话保持一致。当客户端会话失效，该节点自动清理。 
>
> 临时有序节点 ： 在临时节点上多了一个顺序性的特性。

#### 2.4 zookeeper的命令操作

**`1. 查看zookeeper客户端的命令`**

> <img src="D:\data\typora-images\image-20200418130209331.png" alt="image-20200418130209331" style="zoom:80%;" />



**`2. create [-s] [-e] path data acl`**

> `-s 表示节点是否有序`
>
> `-e 表示是否为临时节点`
>
> `默认情况下，是持久化节点`
>
> ![image-20200418125833020](D:\data\typora-images\image-20200418125833020.png)
>
> ![image-20200418125940613](D:\data\typora-images\image-20200418125940613.png)



**`3. get path [watch]`**

> 获得指定path的信息
>
> ![image-20200418125703530](D:\data\typora-images\image-20200418125703530.png)

**`4. set path data [version]`**

> 修改节点path对应的data
>
> 数据库里面有一个version字段去控制数据行的版本号
>
> version 当作乐观锁；
>
> 
>
> ![image-20200418130709005](D:\data\typora-images\image-20200418130709005.png)

**`5. delete path [version]`**

删除节点



**`6. stat信息`**

~~~shell
[zk: localhost:2181(CONNECTED) 22] get /liyanchao
value:lyc
cZxid = 0x200000002 #节点被创建时的事务ID 
ctime = Sat Apr 18 12:46:28 CST 2020
mZxid = 0x200000005 #节点最后一次被更新的事务ID
mtime = Sat Apr 18 13:06:07 CST 2020
pZxid = 0x200000002 #当前结点下的子节点最后一次被修改的事务ID
cversion = 0  #子节点的版本号
dataVersion = 1 #表示当前节点数据版本号
aclVersion = 0  #表示acl的版本号，修改节点权限
ephemeralOwner = 0x0　#创建临时节点的时候，会有一个sessionId,该值存储的就是这个sessionId.
dataLength = 9 #数据值长度
numChildren = 0 #子节点数量
~~~



#### 2.5 zookeeper的Watcher与权限

Watcher

> zookeeper提供了分布式数据发布/订阅,zookeeper允许客户端想服务器注册一个watcher监听.当服务器端的节点触发指定事件的时候,会触发watcher.服务端会向客户端发送一个事件通知.
>
> watcher的通知是一次性,一旦出发一次通知后,该watcher就失效.

ACL

> zookeeper提供控制节点访问权限的功能,用于有效的保证zookeeper中数据的安全性.避免误操作而导致系统出现重大事故.
>
> 权限种类：create /read/write/delete/admin



**连接状态：**

> KeeperStat.Expired 在一定时间内，客户端没有收到服务器的通知，则认为当前的绘画已经过期了。
>
> KeeperStat.Disconnected 断开连接的状态
>
> KeeperStat.SyncConected 客户端和服务器端在某一个节点上建立连接，并且完成一次version/zxid同步。
>
> KeeperStat.authFailed  授权失败



**事件类型：**

> NodeCreated  当节点被创建的时候，触发。
>
> NodeChildrenChanged 表示子节点被创建/被删除/子节点数据发生变化
>
> NodeDataChange 节点数据发生变化
>
> NodeDeleted 节点被删除
>
> Node 客户端和服务器端连接状态发生变化的时候，事件类型就是None



**Curator**

> Curator本身是Netflix公司开源的zookeeper客户端。
>
> curator提供了各种应用场景的实现封装。
>
> curator-framework 提供工了fluent风格api（链式编程）
>
> curator-replice 提供了实现封装



**curator连接的重试策略**

> ExponentialBackoffRetry 衰减重拾
>
> RetryNTimes 指定最大重试次数
>
> RetryOneTime 仅重试一次
>
> RetryUnitilElapsed 一直重试直到规定的时间



**三种watcher来做节点的监听**

> * pathcache   监视一个路径下子节点的创建、删除、节点数据更新
> * NodeCache   监视一个节点的创建、更新、删除
> * TreeCache   pathcaceh+nodecache 的合体（监视路径下的创建、更新、删除事件），缓存路径下的所有子节点的数据



### 3. zookeeper的实际应用场景

#### 3.1 数据发布订阅/配置中心

> * 实现配置中心有两种模式：push（服务器端主动发起请求获取配置中心）/pull（客户端主动发起请求获取配置中心）。
>
> * zookeeper使用的是（推/拉）相结合的方式，客户端向服务器端注册自己所需要关注的节点，一旦节点发生变化，服务器端就会向相应的客户端发送watch通知。客户端收到通知后，主动到服务器端获取更新后的数据。
>
> * 特点：
>
>   1.数据量比较小
>
>   2.数据内容在运行时会发生动态变更
>
>   3.集群中的各个机器共享配置



#### 3.2 分布式锁

**实现分布式锁的几中方式**：

1.redis可实现。 

​		setNX 存在则会返回0，

2.数据库方式实现(两种方式)

​		2.1 创建一个表，通过索引唯一的方式实现

​				create table(id,methodname...)  methodname 增加唯一索引

​				insert 一条数据XXX，成功的话，返回1，失败的无法获取锁。获取锁执行业务完毕，delete语句删除这				条记录。

​		2.2 MySQL Innodb 引擎，通过for update语句（行锁/表锁），使其他线程无法获取锁。

3.zookeeper实现分布式锁

​		zookeeper拥有排它锁/共享锁：

排它锁

​		 所有客户端都去locks节点上去写相同的临时子节点，根据zookeeper原理只能有一个客户端写成功，其他客户端写失败，来实现排它锁。 

共享锁（读锁）

​		所有客户端都去locks节点上去写临时有序子节点：

​    	客户端A要做读事务的操作，就写read-lock*的临时有序节点-->/read-lock0000000001

​    	客户端B要做读事务的操作，就写read-lock*的临时有序节点-->/read-lock0000000002

​    	客户端C要做写事务的操作，就写read-lock*的临时有序节点-->/write-lock0000000003

​    	然后这些客户端请求，就根据/locks下的临时有序节点，按照序号最小的开始递增的执行拥有/locks子节点的客户端的请求，来实现。



> ![image-20200419173643197](D:\data\typora-images\image-20200419173643197.png)



zookeeper实现分布式锁

> ​        ![image-20200419164235253](D:\data\typora-images\image-20200419164235253.png)





#### 3.3 zookeeper实现负载均衡

> 后端服务器在zookeeper上的某个节点上注册子节点，当客户端访问后端服务器时，zookeeper通过负载均衡算法，将指定节点上的一个子节点，然后将请求路由到子节点所属的后端服务器上。



> ID生成器
>
> 分布式队列
>
> 统一命名服务



#### 3.4 zookeeper实现master选举

> 7*24h可用，99.9999%可用，由此使用master-slave模式（主从），正常情况下，只有master提供服务，只有当master出现故障的时候，slave才会工作（上位-->master），之前的master修复好之后，就会变为slave；
>
> 当slave选举成为master之后，就会出现两个master，由此，需要zookeeper来解决；
>
> ![image-20200419193619876](D:\data\typora-images\image-20200419193619876.png)



#### 3.5 zookeeper 权限控制模式

> schema (授权对象)，有如下四种方式：
>
> * IP : 192.168.2.8,通过指定IP的形式方问zookeeper.
>
> * Digest : username:password ;通过用设置 用户名密码方式,方问zookeeper.
>
> * world : 开放式的权限控制模式,数据节点的访问权限对所有用户开放.  world--->anyone
>
> * super : 超级用户,可以对zookeeper上的数据节点进行操作.



#### 3.6 zookeeper集群环境

`zookeeper集群包含三种角色：leader/follower/observer`

zookeeper一般是由2n+1台机器组成；

##### 3.6.1 集群角色

* **Leader ：** 

  1.事务请求的唯一调度和处理者，保证集群事务处理的顺序性。

  2.集群内部各个服务器的调度者

  接受所有follower的提案请求并统一协调转发提案的投票，负责与所有的follower进行内部的数据交				换（同步）；

​		leader接收所有写请求，由leader转发给所有follower节点，

​		Leader -->中心化思想中的领导。

* **Follower ：**

  1.处理客户端非事务请求，以及转发事务请求给leader服务器；

  2.参与事务请求提议的投票（客户端的一个事务请求，需要半数服务器投票通过以后次啊能通知leader        commit；leader会发起一个提案，要求follower投票）；

  3.参与leader选举的投票

​		直接为客户端服务并参与提案的投票，同时与Leader进行数据交换（同步）；

* **Observer ：**

​		直接为客户端服务但并不参与提案的投票，同时也与leader进行数据交换（同步）；

​        1.观察zookeeper集群中最新状态的变化并将这些状态同步到observer服务器上。

​		2.增加observer不影响集群中事务处理能力，同时还能提升集群的非事务处理能力。（observer接收用户写请求，同时如果是事务请求，会转发给leader；但不参与投票（事务投票/选举leader投票）；）



###### 3.6.1.1 Observer介绍

> observer是一种特殊的zookeeper节点。可以帮助解决zookeeper的扩展性（如果大量客户端访问我们zookeeper集群，需要增加zookeeper集群机器数量，从而增加zookeeper集群的性能。
>
> 集群节点增多，导致zookeeper写性能下降，zookeeper的数据节点变更需要半数以上服务器投票通过，投票就会造成网络消耗增加投票成本）
>
> 1.observer不参与投票，只接受投票结果。
>
> 2.observer不属于zookeeper的关键部位。observer down掉之后，不影响zookeeper

###### 3.6.1.2 Observer配置

`如果集群中添加observer节点，需要在此节点zoo.cfg中添加如下配置。`

1.在zoo.cfg里面增加配置

#设置本台机器为observer
peerType=observer

2.针对设置哪台服务器当作observer，就做如下操作

> server.1=192.168.126.130:2888:3181
>
> server.2=192.168.126.131:2888:3181
>
> server.3=192.168.126.132:2888:3181
>
> server.4=192.168.126.133:2888:3181**:observer**



##### 3.6.2 zookeeper集群示意图：

![image-20200418121552819](D:\data\typora-images\image-20200418121552819.png)



#### 3.7 zookeeper选举Leader

zookeeper选举Leader的算法：(leaderElection/AuthFastLeaderElection/FastLeaderElection)

> 默认情况下，选择 **FastLeaderElection** 算法选举leader；
>
> **serverid**：在配置server集群的时候，给定服务器的标识id（myid）
>
> **zxid**：服务器在运行时产生的数据ID，zxid的值越大，表示数据越新
>
> **Epoch**：选举的轮数
>
> **server的状态：Looking /Following/Observering/Leading**



 **服务器第一次初始化启动的时候：Looking**

1. 所有在集群中的server都会推荐自己为leader，然后把（myid/zxid/epoch）作为广播信息，广播给集群中的其他server，然后等待其他服务器返回。
2. 每个服务器都会接收来自集群中的其他服务器的投票。集群中的每个服务器在接受到投票后，开始判断投票的有效性。
   1. 判断逻辑始终（Epoch），如果Epoch大于自己当前的Epoch，说明自己保存的Epoch是过期的。更新Epoch，同时clear其他服务器发送过来的              新当前自己的选举情况。
   2. 如果Epoch小于目前的Epoch，说明对方的Epoch过期了，也就意味着对方服务器的选举轮数是过期的。这个时候，只需要将自己的信息发送给对方
   3. 如果sid大于目前的sid，根据规则来判断是否有资格获得leader
      1. 接收到来自其他服务器的投票后，针对每一个投票，都需要将别人的投票和自己的投票进行PK（ZXID,最大的服务器优先）



**zookeeper选举leader示意图**

> 左图为zookeeper启动时选举leader
>
> 右图为leader挂掉后，选举leader

![image-20200420082306846](D:\data\typora-images\image-20200420082306846.png)



#### 3.8 ZAB协议

**拜占庭问题**:

> paxos协议主要是如何保证在分布式环境网络下，各个服务器如何达成一致最终保证数据的一致性问题。
>
> ZAB协议，基于paxos协议的一个改进。
>
> ZAB协议为分布式协调服务zookeeper专门设计的一种支持崩溃恢复的原子广播协议
>
> zookeeper并没有完全采用paxos算法，而是采用zab Zookeeper atomic broadcast



**ZAB协议原理**

* 1.在zookeeper的主备模式下，通过zab协议来保证集群中各个副本数据的一致性。

* 2.zookeeper使用的是单一主进程来接收并处理所有的事务请求，并采用zab协议，把数据的状态变更以事务请求的形式广播到其他的节点

* 3.zab协议在主备模型架构中，保证了同一时刻只能有一个主进程来广播服务器的状态变更

* 4.所有的事务请求必须由全局唯一的服务器来协调处理，这个服务器叫leader，其他的叫follower。

​	   leader节点主要负责把客户端的事务请求转化成一个事务提议（proposal），并分发给集群中的所有follower节点，在等待所有follower节点的反馈。一旦超过半数服务器进行了正确的反馈，那么leader就会commit这条消息。



**ZAB协议的工作原理**

> * 1.什么情况下zab协议会进入崩溃风恢复模式
>   * 1.当服务器启动时
>   * 2.当leader出现网络中断/崩溃/重启的情况
>   * 3.集群中已经不存在过半的服务器与该leader保持正常的通信
>
> * 2.zab协议进入崩溃恢复模式会作什么
>
>   * 1.当leader出现问题，zab协议进入崩溃恢复模式，并且选举出新的leader。当前读的leader选举出来以后，如果集群中已经有过半机器完成了leader服务器的状态同步（数据同步），退出本崩溃恢复，进入消息广播模式。
>
>     2.当新的机器加入到集群中的时候，如果已经存在leader服务器，那么新加入的服务器就会自觉进入数据恢复模式，找到leader进行数据同步。



**问题，结合下图事务请求提交流程**

<img src="D:\data\typora-images\image-20200420092548335.png" alt="image-20200420092548335" style="zoom:65%;" />

**假设一个事物在leader服务器被提交了，并且已经有过半的follower返回了ack。在leader节点把commit消息发送给folower机器之前，leader服务器挂了怎么办？**

> zab协议一定需要保证已经被leader提交的事务也能够被所有follower提交。
>
> zab协议需要保证，在崩溃恢复过哦城中跳过那些已经被丢弃的事务。

zab协议需要保证，在崩溃恢复过哦城中跳过那些已经被丢弃的事务。





#### 2.3 zookeeper单机安装

​		1.下载zookeeper的安装包：

​				 http://archive.apache.org/dist/zookeeper/zookeeper-3.4.10/

​		2.解压zookeeper： 

~~~shell
	tar -zxvf zookeeper-3.4.10.tar.gz
~~~

​		3.cd到zookeeper安装目录下的conf下

~~~ shell
	cd /data/program/zookeeper-3.4.10/conf
	cp zoo_sample.cfg zoo.cfg
~~~

​		4.运行zookeeper,

~~~shell
	sh zkServer.sh start
~~~

​		5.客户端连接zookeeper : 	sh zkCli.sh -server ip:port

~~~shell
	sh zkCli.sh -server localhost:2181
~~~







#### 2.5 集群搭建

`192.168.126.130 / 192.168.126.131 / 192.168.126.132 / 192.168.126.133`

`192.168.126.133当作observer节点`

**第一步，修改配置文件**

server.id=host:port():port(如果leader挂了，需要一个端口来重新选举)    【id的取值范围：1~255；用id来标识该机器在集群中的机器序号】

> server.1=192.168.126.130:2888:3181
>
> server.2=192.168.126.131:2888:3181
>
> server.3=192.168.126.132:2888:3181
>
> server.4=192.168.126.133:2888:3181:observer

2888：表示follower节点与leader节点交换信息的端口号；（可自定义）

3181：是leader选举的端口号；

2181：是客户端连接zookeeper的端口号；

**第二步，创建myid**

在每个服务器的dataDir目录下创建一个myid的文件，文件就一行数据，数据内容是每台机器对应的serverID的数字

~~~shell
vim /data/program/zookeeper-3.4.10/conf/zoo.cfg
~~~

得到 dataDir目录 值： `dataDir=/tmp/zookeeper`

~~~shell
cd /tmp/zookeeper
vim myid
~~~

对应服务器加入1  2  3  4，wq保存退出。



**zoo.cfg配置文件分析**

~~~shell
#zookeeper中最小的时间单位长度（ms）
tickTime=2000 

#follower节点启动后与leader节点完成同步的时间
initLimit=10  

#leader节点和follower节点进行心跳检测的最大延迟时间，如果时间到了还没接收到follower的心跳，则表示#follower已经失去了。
syncLimit=5

#zookeeper服务器存储快照文件，是一个备份的目录
dataDir=/tmp/zookeeper

#配置zookeeper事务日志存储路径，默认指定在dataDir目录下;
#dataLogDir一般不与dataDir挂载到同一个磁盘，因为会影响到事务日志的写性能。
dataLogDir=路径

#客户端连接zookeeper的端口号
clientPort=2181
~~~

**最终4台服务器 zoo.cfg 配置如下：**

> 第一台机器配置（192.168.126.130）：![image-20200418115301192](D:\data\typora-images\image-20200418115301192.png)
>
> 第二台机器配置（192.168.126.131）：![image-20200418115505578](D:\data\typora-images\image-20200418115505578.png)
>
> 第二台机器配置（192.168.126.132）：![image-20200418115505578](D:\data\typora-images\image-20200418115505578.png)
>
> 第二台机器配置（192.168.126.133）：![image-20200418115533341](D:\data\typora-images\image-20200418115533341.png)



第三步：关闭防火墙， 禁止firewall开机启动 。

`三台机器都做此操作。`

~~~shell
systemctl stop firewalld.service
systemctl disable firewalld.service 
~~~

第四步：启动zookeeper

~~~shell
sh /data/program/zookeeper-3.4.10/bin/zkServer.sh start
~~~







