#  zookeeper分布式锁原理与zab协议



### 1. zookeeper的实际应用场景

#### 1.1 数据发布订阅/配置中心

> * 实现配置中心有两种模式：push（服务器端主动发起请求获取配置中心）/pull（客户端主动发起请求获取配置中心）。
>
> * zookeeper使用的是（推/拉）相结合的方式，客户端向服务器端注册自己所需要关注的节点，一旦节点发生变化，服务器端就会向相应的客户端发送watch通知。客户端收到通知后，主动到服务器端获取更新后的数据。
>
> * 特点：
>
>   1.数据量比较小
>
>   2.数据内容在运行时会发生动态变更
>
>   3.集群中的各个机器共享配置



#### 1.2 分布式锁

**实现分布式锁的几中方式**：

1.redis可实现。 

​		setNX 存在则会返回0，

2.数据库方式实现(两种方式)

​		2.1 创建一个表，通过索引唯一的方式实现

​				create table(id,methodname...)  methodname 增加唯一索引

​				insert 一条数据XXX，成功的话，返回1，失败的无法获取锁。获取锁执行业务完毕，delete语句删除这				条记录。

​		2.2 MySQL Innodb 引擎，通过for update语句（行锁/表锁），使其他线程无法获取锁。

3.zookeeper实现分布式锁

​		zookeeper拥有排它锁/共享锁：

排它锁

​		 所有客户端都去locks节点上去写相同的临时子节点，根据zookeeper原理只能有一个客户端写成功，其他客户端写失败，来实现排它锁。 

共享锁（读锁）

​		所有客户端都去locks节点上去写临时有序子节点：

​    	客户端A要做读事务的操作，就写read-lock*的临时有序节点-->/read-lock0000000001

​    	客户端B要做读事务的操作，就写read-lock*的临时有序节点-->/read-lock0000000002

​    	客户端C要做写事务的操作，就写read-lock*的临时有序节点-->/write-lock0000000003

​    	然后这些客户端请求，就根据/locks下的临时有序节点，按照序号最小的开始递增的执行拥有/locks子节点的客户端的请求，来实现。



> ![image-20200419173643197](D:\data\typora-images\image-20200419173643197.png)



zookeeper实现分布式锁

> ​        ![image-20200419164235253](D:\data\typora-images\image-20200419164235253.png)





#### 1.3 zookeeper实现负载均衡

> 后端服务器在zookeeper上的某个节点上注册子节点，当客户端访问后端服务器时，zookeeper通过负载均衡算法，将指定节点上的一个子节点，然后将请求路由到子节点所属的后端服务器上。



> ID生成器
>
> 分布式队列
>
> 统一命名服务



#### 1.4 zookeeper实现master选举

> 7*24h可用，99.9999%可用，由此使用master-slave模式（主从），正常情况下，只有master提供服务，只有当master出现故障的时候，slave才会工作（上位-->master），之前的master修复好之后，就会变为slave；
>
> 当slave选举成为master之后，就会出现两个master，由此，需要zookeeper来解决；
>
> ![image-20200419193619876](D:\data\typora-images\image-20200419193619876.png)



#### 1.5 zookeeper 权限控制模式

> schema (授权对象)，有如下四种方式：
>
> * IP : 192.168.2.8,通过指定IP的形式方问zookeeper.
>
> * Digest : username:password ;通过用设置 用户名密码方式,方问zookeeper.
>
> * world : 开放式的权限控制模式,数据节点的访问权限对所有用户开放.  world--->anyone
>
> * super : 超级用户,可以对zookeeper上的数据节点进行操作.



#### 1.6 zookeeper集群环境

`zookeeper集群包含三种角色：leader/follower/observer`

zookeeper一般是由2n+1台机器组成；

##### 1.6.1 集群角色

* **Leader ：** 

  1.事务请求的唯一调度和处理者，保证集群事务处理的顺序性。

  2.集群内部各个服务器的调度者

  接受所有follower的提案请求并统一协调转发提案的投票，负责与所有的follower进行内部的数据交				换（同步）；

​		leader接收所有写请求，由leader转发给所有follower节点，

​		Leader -->中心化思想中的领导。

* **Follower ：**

  1.处理客户端非事务请求，以及转发事务请求给leader服务器；

  2.参与事务请求提议的投票（客户端的一个事务请求，需要半数服务器投票通过以后次啊能通知leader        commit；leader会发起一个提案，要求follower投票）；

  3.参与leader选举的投票

​		直接为客户端服务并参与提案的投票，同时与Leader进行数据交换（同步）；

* **Observer ：**

​		直接为客户端服务但并不参与提案的投票，同时也与leader进行数据交换（同步）；

​        1.观察zookeeper集群中最新状态的变化并将这些状态同步到observer服务器上。

​		2.增加observer不影响集群中事务处理能力，同时还能提升集群的非事务处理能力。（observer接收用户写请求，同时如果是事务请求，会转发给leader；但不参与投票（事务投票/选举leader投票）；）



###### 1.6.1.1 Observer介绍

> observer是一种特殊的zookeeper节点。可以帮助解决zookeeper的扩展性（如果大量客户端访问我们zookeeper集群，需要增加zookeeper集群机器数量，从而增加zookeeper集群的性能。
>
> 集群节点增多，导致zookeeper写性能下降，zookeeper的数据节点变更需要半数以上服务器投票通过，投票就会造成网络消耗增加投票成本）
>
> 1.observer不参与投票，只接受投票结果。
>
> 2.observer不属于zookeeper的关键部位。observer down掉之后，不影响zookeeper

###### 3.6.1.2 Observer配置

`如果集群中添加observer节点，需要在此节点zoo.cfg中添加如下配置。`

1.在zoo.cfg里面增加配置

#设置本台机器为observer
peerType=observer

2.针对设置哪台服务器当作observer，就做如下操作

> server.1=192.168.126.130:2888:3181
>
> server.2=192.168.126.131:2888:3181
>
> server.3=192.168.126.132:2888:3181
>
> server.4=192.168.126.133:2888:3181**:observer**



##### 1.6.2 zookeeper集群示意图：

![image-20200418121552819](D:\data\typora-images\image-20200418121552819.png)



#### 1.7 zookeeper选举Leader

zookeeper选举Leader的算法：(leaderElection/AuthFastLeaderElection/FastLeaderElection)

> 默认情况下，选择 **FastLeaderElection** 算法选举leader；
>
> **serverid**：在配置server集群的时候，给定服务器的标识id（myid）
>
> **zxid**：服务器在运行时产生的数据ID，zxid的值越大，表示数据越新
>
> **Epoch**：选举的轮数
>
> **server的状态：Looking /Following/Observering/Leading**



 **服务器第一次初始化启动的时候：Looking**

1. 所有在集群中的server都会推荐自己为leader，然后把（myid/zxid/epoch）作为广播信息，广播给集群中的其他server，然后等待其他服务器返回。
2. 每个服务器都会接收来自集群中的其他服务器的投票。集群中的每个服务器在接受到投票后，开始判断投票的有效性。
   1. 判断逻辑始终（Epoch），如果Epoch大于自己当前的Epoch，说明自己保存的Epoch是过期的。更新Epoch，同时clear其他服务器发送过来的              新当前自己的选举情况。
   2. 如果Epoch小于目前的Epoch，说明对方的Epoch过期了，也就意味着对方服务器的选举轮数是过期的。这个时候，只需要将自己的信息发送给对方
   3. 如果sid大于目前的sid，根据规则来判断是否有资格获得leader
      1. 接收到来自其他服务器的投票后，针对每一个投票，都需要将别人的投票和自己的投票进行PK（ZXID,最大的服务器优先）



**zookeeper选举leader示意图**

> 左图为zookeeper启动时选举leader
>
> 右图为leader挂掉后，选举leader

![image-20200420082306846](D:\data\typora-images\image-20200420082306846.png)



### 2. ZAB协议

**拜占庭问题**:

> paxos协议主要是如何保证在分布式环境网络下，各个服务器如何达成一致最终保证数据的一致性问题。
>
> ZAB协议，基于paxos协议的一个改进。
>
> ZAB协议为分布式协调服务zookeeper专门设计的一种支持崩溃恢复的原子广播协议
>
> zookeeper并没有完全采用paxos算法，而是采用zab Zookeeper atomic broadcast



**ZAB协议原理**

* 1.在zookeeper的主备模式下，通过zab协议来保证集群中各个副本数据的一致性。

* 2.zookeeper使用的是单一主进程来接收并处理所有的事务请求，并采用zab协议，把数据的状态变更以事务请求的形式广播到其他的节点

* 3.zab协议在主备模型架构中，保证了同一时刻只能有一个主进程来广播服务器的状态变更

* 4.所有的事务请求必须由全局唯一的服务器来协调处理，这个服务器叫leader，其他的叫follower。

​	   leader节点主要负责把客户端的事务请求转化成一个事务提议（proposal），并分发给集群中的所有follower节点，在等待所有follower节点的反馈。一旦超过半数服务器进行了正确的反馈，那么leader就会commit这条消息。



**ZAB协议的工作原理**

> * 1.什么情况下zab协议会进入崩溃风恢复模式
>   * 1.当服务器启动时
>   * 2.当leader出现网络中断/崩溃/重启的情况
>   * 3.集群中已经不存在过半的服务器与该leader保持正常的通信
>
> * 2.zab协议进入崩溃恢复模式会作什么
>
>   * 1.当leader出现问题，zab协议进入崩溃恢复模式，并且选举出新的leader。当前读的leader选举出来以后，如果集群中已经有过半机器完成了leader服务器的状态同步（数据同步），退出本崩溃恢复，进入消息广播模式。
>
>     2.当新的机器加入到集群中的时候，如果已经存在leader服务器，那么新加入的服务器就会自觉进入数据恢复模式，找到leader进行数据同步。



**问题，结合下图事务请求提交流程**

<img src="D:\data\typora-images\image-20200420092548335.png" alt="image-20200420092548335" style="zoom:65%;" />

**假设一个事物在leader服务器被提交了，并且已经有过半的follower返回了ack。在leader节点把commit消息发送给folower机器之前，leader服务器挂了怎么办？**

> zab协议一定需要保证已经被leader提交的事务也能够被所有follower提交。
>
> zab协议需要保证，在崩溃恢复过哦城中跳过那些已经被丢弃的事务。





