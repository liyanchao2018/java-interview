# 第一种方式：

# [如何从文件大小大约是 100GB的文件中，海量IP中提取出现最多的10个IP](https://www.cnblogs.com/huangzs/p/10245976.html)             



# 场景

这是一个 ip 地址 127.0.0.1，**每行一个ip**。
 假设有100亿个这样的 ip 地址存在文件中
 这个文件大小大约是 100GB
 **问题：要统计出100亿个 ip 中，重复出现次数最多的前10个**

# 分析

1.100GB 几乎不可能一次加载进内存进行操作，所以必须要拆分
 那么可以利用分治的思想，把规模大的问题化小，然后解决各个小的问题，最后得出结果。

2.我的理解：

1.设计一个hash函数，先将100GB大文件，文件中的IP % 10000，得到10000份

hash值相同的小文件

2.每个小文件，通过HashMap，记录key=ip，value=出现的次数。然后统计出当前文件中出现次数最多的ip。

3.将多个小文件，汇总，得出出现次数最多的前10个ip。



# 实现思路

- ipv4 地址是一个 32 位的整数，可以用 uint 保存。
- 我先设计一个哈希函数，把100个G的文件分成10000份，每份大约是 10MB，可以加载进内存了。

> 例如：我设计一个简单的哈希函数是 f(ip) = ip % 10000，(ip 是个32位整数)
>  那么 5 % 10000 = 5，不管 5 在哪个地方 5 % 10000 的结果都是 5，这就保证了相同的 ip 会被放在同一个子文件中，方便统计，相同的元素经过同一个哈希函数，得出的哈希值是一样的。
>  那么我把100亿个 ip，都进行 ip % 10000 的操作，就完成了 100GB 文件分解成 10000个子文件的任务了。当然实际中哈希函数的选取很重要，尽量使得元素分布均匀，哈希冲突少的函数才是最好的。

记住，我把上面这个分解的过程叫做 **Map**，由一台叫 **master** 的计算机完成这个工作。

- 10MB 的小文件加进内存，统计出出现次数最多的那个ip

> 1.10MB 的小文件里面存着很多 ip，他们虽然是乱序的，但是相同的 ip 会映射到同一个文件中来！
>  那么可以用二叉树统计出现次数，二叉树节点保存（ip, count）的信息，把所有 ip 插入到二叉树中，如果这个 ip  不存在，那么新建一个节点, count 标记 1，如果有，那么把 count++，最终遍历一遍树，就能找出 count 最大的 ip 了。【问题：Java如何使用二叉树】
>
> 2.此处，还有另外一种方式：
>
>    **对于每一个小文件，可以构建一个IP为key，出现的次数为value的Hash Map，同时记录当前出现次数最多的那个IP地址；** 

我把这个过程叫做 **Reduce**，由很多台叫 **worker** 的计算机来完成。
 每个 worker 至少要找出最大的前10个 ip 返回给 master，master 最后会收集到 10000 * 10 个 ip，大约 400KB，然后再找出最大的前 10 个 ip 就可以了。
 最简单的遍历10遍，每次拿个最大值出来就可以了，或者用快速排序，堆排序，归并排序等等方法，找出最大前 k 个数也行。

# MapReduce

我刚刚除了介绍了一种海量数据的哈希分治算法之外，还穿插了一个谷歌的 MapReduce 分布式并行编程模型，原理就是上面说的那些了，有兴趣的可以去详细了解。

哈希函数是什么？哈希函数是把大空间的元素映射到一个小空间里。

说完了原理，我已经根据上面的原理写了一个实验程序，有兴趣的可以去看看，地址在 [这里](https://link.jianshu.com?t=https://github.com/hehe520/Data-structure-and-algorithm/blob/master/海量数据处理/重复度最高的IP - 哈希分治.cpp)
 可以下载来看，代码是C++的，vs2008 编译环境。